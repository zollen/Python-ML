url: https://www.youtube.com/channel/UC59tJcvdkXSpl_-_kUZbjew/videos

Stationary time series is when the mean and variance are constant over time. 
Differencing is a method of transforming a non-stationary time series into a stationary one. 
This is an important step in preparing data to be used in an ARIMA model.

The first differencing value is the difference between the current time period and the previous time period.
If these values fail to revolve around a constant mean and variance then we find the second differencing 
using the values of the first differencing. We repeat this until we get a stationary series

The best way to determine whether or not the series is sufficiently differenced is to plot the differenced 
series and check to see if there is a constant mean and variance.

Autocorrelation function (ACF)
==============================
Autocorrelation refers to how correlated a time series is with its past values whereas the ACF is the plot 
used to see the correlation between the points, up to and including the lag unit. In ACF, the correlation 
coefficient is in the x-axis whereas the number of lags is shown in the y-axis.

Normally in an ARIMA model, we make use of either the AR term or the MA term. We use both of these terms 
only on rare occasions. We use the ACF plot to decide which one of these terms we would use for our time 
series
If there is a Positive autocorrelation at lag 1 then we use the AR model
If there is a Negative autocorrelation at lag 1 then we use the MA model


AutoRegressive Model (AR)
=========================
AR - forecast a series based solely on the past values in the series called lags.

Y(t)     = target
Y(t - 1) = lagged target
e(t)     = Error

Y(t)   = w + φ Y(t-1) + e(t)
Y(t-1) = w + φ Y(t-2) + e(t-1)
...
Y(t)   = w + φ Y(w + φ Y(t-2) + e(t-1)) + e(t) 
Y(t)   = w* + φ^2 Y(t-2) + φ e(t-1) + e(t)
...
Recursivly substitute Y-1, Y-2, Y-3....1
...
Y(t)       = w / (1 - w) + φ1^t Y(1) + φ2^(t-1) e(2) + φ3^(t-2) e(3) + ... + e(t)

Predict(t) = φ0 + φ1^t Y(1) + φ2^(t-1) e(2) + φ3^(t-2) e(3) + ... + e(t)

PACF mesaures the correlation of the direct affect between two periods
φ1, φ2, φ3..etc = PACF (partial auto-correlation function, we pick φ# for the AR equation only if the φ# exceeds the threshold)

Above equaion can link by to the Y(1). This is also called Long memory models
So the effect of shocks that happen long ago have little effect on the present *if* |φ| < 1
The first observation(Y(1) still matter but it has very very small impact to the prediction of Y(t)


Moving Average Models (MA)
==========================
MA - forecast a series based solely on the past errors in the series called error lags.

w		 = mean/average
Y(t)     = target
Y(t - 1) = lagged Error
e(t)     = Error

Y(t+1)   = w + φ e(t) + e(t+1)
Y(t)     = w + φ e(t-1) + e(t)
Y(t-1)   = w + φ e(t-2) + e(t-1)
Y(t-2)   = w + φ e(t-3) + e(t-2)

Predict(t)     = w + φ1 e(t - 1) + φ2 e(t - 2) + φ3 e(t - 3) + ...
Actual(t)      = w + φ1 e(t - 1) + φ2 e(t - 2) + φ3 e(t - 3) + ... + e(t)

Above question can only link to e(t-1) and no further, This is called Short memory models

ACF mesaures the correlation of both the direct & indirect affect between two periods
φ1, φ2, φ3..etc = ACF (Auto-correlation function, we pick φ# for the MA equation only if the φ# exceeds the threshold)


Combing both AR and MA (ARMA)
=============================
AR(1)
Y(t)   = w / (1 - w) + φ1^t Y(1) + φ2^(t-1) e(2) + φ3^(t-2) e(3) + ... + e(t)
                                |----------------- MA(∞) ----------------|

Actual1(t)  = φ0 + φ(AR:1) Y(t-1) + φ(MA:1) e(t-1) + e(t)
Predict1(t) = φ0 + φ(AR:1) Y(t-1) + φ(MA:1) e(t-1)

Predict(t) would not have the current error term because it has not happen yet.

How many AR terms and how many MA term should we use.
1. Plotting patterns in correlation
2. Automatic selection techniques
	There are three selection techniques (MINIC, SCAN, ESACF)


ARIMA  (I stands for integrated)
================================
Essentially, you need to make your data stationary (typically done through differncing)
Distriubtion depends *only* on difference in time, *not* location in time

ARIMA(p, d, q)
p - # of AR terms
d - # of first differences
q - # of MA terms

Y(t) - Y(t-1) = W(t) <-- difference between Y(t) and Y(t-1)
W(t) = w + φ(AR:1) W(t-1) + φ(MA:1) e(t-1) + e(t)

W(t-1) - p
W(t)   - d
e(t-1) - q


What is seasonality?
====================
A repeating pattern within a year
We need to remove seasonality so we can apply the models
Z(t) = y(t+265) - y(t)

Cycle is a trend span across mutliple years. Cycle is not as predictable as seasonality


Invertibility
=============
AR(1) = MA(∞) when φ < 1

C(t) = φ C(t-1) + e(t)
(1 - φL) C(t) = e(t)
   1
-------- e(t) = c(t)  
(1 - φL) 
φ must be < 1 so we can transform 1 / (1 - φL) into a infinite geometric series
(1 + φL + φ^2L^2 +....) e(t) = c(t)
φ e(t-1) + φ^2 e(t-2) + φ^3 e(t-3) + .... = MA(∞) = c(t)
